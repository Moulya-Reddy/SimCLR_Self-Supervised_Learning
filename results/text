Files already downloaded and verified
Epoch 1/10 started...
Batch 0/196 - Loss: 4.7587
Batch 10/196 - Loss: 3.6527
Batch 20/196 - Loss: 3.6210
Batch 30/196 - Loss: 3.6169
Batch 40/196 - Loss: 3.6151
Batch 50/196 - Loss: 3.6208
Batch 60/196 - Loss: 3.6134
Batch 70/196 - Loss: 3.6133
Batch 80/196 - Loss: 3.6153
Batch 90/196 - Loss: 3.6191
Batch 100/196 - Loss: 3.6067
Batch 110/196 - Loss: 3.6130
Batch 120/196 - Loss: 3.6119
Batch 130/196 - Loss: 3.6143
Batch 140/196 - Loss: 3.6115
Batch 150/196 - Loss: 3.6083
Batch 160/196 - Loss: 3.6090
Batch 170/196 - Loss: 3.6103
Batch 180/196 - Loss: 3.6049
Batch 190/196 - Loss: 3.6058
Epoch [1/10] completed. Loss: 710.3191
Epoch 2/10 started...
Batch 0/196 - Loss: 3.6055
Batch 10/196 - Loss: 3.6037
Batch 20/196 - Loss: 3.6090
Batch 30/196 - Loss: 3.6026
Batch 40/196 - Loss: 3.6015
Batch 50/196 - Loss: 3.6079
Batch 60/196 - Loss: 3.6055
Batch 70/196 - Loss: 3.6051
Batch 80/196 - Loss: 3.6026
Batch 90/196 - Loss: 3.6050
Batch 100/196 - Loss: 3.6022
Batch 110/196 - Loss: 3.6010
Batch 120/196 - Loss: 3.5987
Batch 130/196 - Loss: 3.6008
Batch 140/196 - Loss: 3.6026
Batch 150/196 - Loss: 3.6028
Batch 160/196 - Loss: 3.6001
Batch 170/196 - Loss: 3.6019
Batch 180/196 - Loss: 3.6002
Batch 190/196 - Loss: 3.5977
Epoch [2/10] completed. Loss: 705.1485
Epoch 3/10 started...
Batch 0/196 - Loss: 3.5998
Batch 10/196 - Loss: 3.6050
Batch 20/196 - Loss: 3.6006
Batch 30/196 - Loss: 3.6001
Batch 40/196 - Loss: 3.5972
Batch 50/196 - Loss: 3.6006
Batch 60/196 - Loss: 3.5969
Batch 70/196 - Loss: 3.6025
Batch 80/196 - Loss: 3.6002
Batch 90/196 - Loss: 3.6039
Batch 100/196 - Loss: 3.6014
Batch 110/196 - Loss: 3.5988
Batch 120/196 - Loss: 3.5979
Batch 130/196 - Loss: 3.5986
Batch 140/196 - Loss: 3.6029
Batch 150/196 - Loss: 3.6008
Batch 160/196 - Loss: 3.5978
Batch 170/196 - Loss: 3.6000
Batch 180/196 - Loss: 3.5951
Batch 190/196 - Loss: 3.5971
Epoch [3/10] completed. Loss: 704.5075
Epoch 4/10 started...
Batch 0/196 - Loss: 3.6010
Batch 10/196 - Loss: 3.5960
Batch 20/196 - Loss: 3.5957
Batch 30/196 - Loss: 3.6008
Batch 40/196 - Loss: 3.5958
Batch 50/196 - Loss: 3.5976
Batch 60/196 - Loss: 3.5971
Batch 70/196 - Loss: 3.5956
Batch 80/196 - Loss: 3.5959
Batch 90/196 - Loss: 3.5956
Batch 100/196 - Loss: 3.5956
Batch 110/196 - Loss: 3.5977
Batch 120/196 - Loss: 3.5980
Batch 130/196 - Loss: 3.5995
Batch 140/196 - Loss: 3.5953
Batch 150/196 - Loss: 3.5934
Batch 160/196 - Loss: 3.5964
Batch 170/196 - Loss: 3.5982
Batch 180/196 - Loss: 3.5958
Batch 190/196 - Loss: 3.5995
Epoch [4/10] completed. Loss: 703.9019
Epoch 5/10 started...
Batch 0/196 - Loss: 3.5976
Batch 10/196 - Loss: 3.5976
Batch 20/196 - Loss: 3.5958
Batch 30/196 - Loss: 3.5987
Batch 40/196 - Loss: 3.6020
Batch 50/196 - Loss: 3.5954
Batch 60/196 - Loss: 3.5928
Batch 70/196 - Loss: 3.5947
Batch 80/196 - Loss: 3.5968
Batch 90/196 - Loss: 3.5949
Batch 100/196 - Loss: 3.5992
Batch 110/196 - Loss: 3.5944
Batch 120/196 - Loss: 3.5969
Batch 130/196 - Loss: 3.5948
Batch 140/196 - Loss: 3.5966
Batch 150/196 - Loss: 3.5954
Batch 160/196 - Loss: 3.5985
Batch 170/196 - Loss: 3.5956
Batch 180/196 - Loss: 3.6017
Batch 190/196 - Loss: 3.5949
Epoch [5/10] completed. Loss: 703.8111
Epoch 6/10 started...
Batch 0/196 - Loss: 3.5975
Batch 10/196 - Loss: 3.5952
Batch 20/196 - Loss: 3.5943
Batch 30/196 - Loss: 3.5959
Batch 40/196 - Loss: 3.5924
Batch 50/196 - Loss: 3.5950
Batch 60/196 - Loss: 3.5944
Batch 70/196 - Loss: 3.5955
Batch 80/196 - Loss: 3.5940
Batch 90/196 - Loss: 3.5952
Batch 100/196 - Loss: 3.5974
Batch 110/196 - Loss: 3.5947
Batch 120/196 - Loss: 3.5944
Batch 130/196 - Loss: 3.5959
Batch 140/196 - Loss: 3.5983
Batch 150/196 - Loss: 3.5949
Batch 160/196 - Loss: 3.5947
Batch 170/196 - Loss: 3.5918
Batch 180/196 - Loss: 3.5938
Batch 190/196 - Loss: 3.5946
Epoch [6/10] completed. Loss: 703.4984
Epoch 7/10 started...
Batch 0/196 - Loss: 3.5918
Batch 10/196 - Loss: 3.5941
Batch 20/196 - Loss: 3.5926
Batch 30/196 - Loss: 3.5993
Batch 40/196 - Loss: 3.5934
Batch 50/196 - Loss: 3.5938
Batch 60/196 - Loss: 3.5971
Batch 70/196 - Loss: 3.5974
Batch 80/196 - Loss: 3.5932
Batch 90/196 - Loss: 3.5937
Batch 100/196 - Loss: 3.5948
Batch 110/196 - Loss: 3.5947
Batch 120/196 - Loss: 3.5923
Batch 130/196 - Loss: 3.5945
Batch 140/196 - Loss: 3.5961
Batch 150/196 - Loss: 3.5961
Batch 160/196 - Loss: 3.5928
Batch 170/196 - Loss: 3.5963
Batch 180/196 - Loss: 3.5946
Batch 190/196 - Loss: 3.5936
Epoch [7/10] completed. Loss: 703.3324
Epoch 8/10 started...
Batch 0/196 - Loss: 3.5965
Batch 10/196 - Loss: 3.5976
Batch 20/196 - Loss: 3.5938
Batch 30/196 - Loss: 3.5922
Batch 40/196 - Loss: 3.5928
Batch 50/196 - Loss: 3.5948
Batch 60/196 - Loss: 3.5938
Batch 70/196 - Loss: 3.5938
Batch 80/196 - Loss: 3.5934
Batch 90/196 - Loss: 3.5941
Batch 100/196 - Loss: 3.5935
Batch 110/196 - Loss: 3.5919
Batch 120/196 - Loss: 3.5931
Batch 130/196 - Loss: 3.5957
Batch 140/196 - Loss: 3.5937
Batch 150/196 - Loss: 3.5929
Batch 160/196 - Loss: 3.5955
Batch 170/196 - Loss: 3.5930
Batch 180/196 - Loss: 3.5921
Batch 190/196 - Loss: 3.5911
Epoch [8/10] completed. Loss: 703.3161
Epoch 9/10 started...
Batch 0/196 - Loss: 3.5937
Batch 10/196 - Loss: 3.5934
Batch 20/196 - Loss: 3.5962
Batch 30/196 - Loss: 3.5946
Batch 40/196 - Loss: 3.5928
Batch 50/196 - Loss: 3.5926
Batch 60/196 - Loss: 3.5927
Batch 70/196 - Loss: 3.5944
Batch 80/196 - Loss: 3.5980
Batch 90/196 - Loss: 3.5961
Batch 100/196 - Loss: 3.5934
Batch 110/196 - Loss: 3.5914
Batch 120/196 - Loss: 3.5914
Batch 130/196 - Loss: 3.5913
Batch 140/196 - Loss: 3.5941
Batch 150/196 - Loss: 3.5916
Batch 160/196 - Loss: 3.5916
Batch 170/196 - Loss: 3.5924
Batch 180/196 - Loss: 3.5956
Batch 190/196 - Loss: 3.5943
Epoch [9/10] completed. Loss: 703.1305
Epoch 10/10 started...
Batch 0/196 - Loss: 3.5942
Batch 10/196 - Loss: 3.5957
Batch 20/196 - Loss: 3.5920
Batch 30/196 - Loss: 3.5965
Batch 40/196 - Loss: 3.5929
Batch 50/196 - Loss: 3.5952
Batch 60/196 - Loss: 3.5943
Batch 70/196 - Loss: 3.5939
Batch 80/196 - Loss: 3.5911
Batch 90/196 - Loss: 3.5916
Batch 100/196 - Loss: 3.5941
Batch 110/196 - Loss: 3.5911
Batch 120/196 - Loss: 3.5922
Batch 130/196 - Loss: 3.5933
Batch 140/196 - Loss: 3.5930
Batch 150/196 - Loss: 3.5923
Batch 160/196 - Loss: 3.5935
Batch 170/196 - Loss: 3.5942
Batch 180/196 - Loss: 3.5903
Batch 190/196 - Loss: 3.5911
Epoch [10/10] completed. Loss: 703.0920
<ipython-input-3-b0c49bc8b564>:127: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load("simclr_model.pth"))
Files already downloaded and verified
